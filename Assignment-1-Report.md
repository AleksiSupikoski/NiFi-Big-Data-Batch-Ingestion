1. 
 I have chosen dataset of tortoises from Korkeasaari Zoo, becase i found it funny and i like turtles. The data is time sieries IoT data coming from ruuvitag devices measuring environmental values and movement of the tortoises. Data is given as separate CSV files on https://iot.fvh.fi/downloads/tortoise/   Data sample: time,readable_time,acceleration,acceleration_x,acceleration_y,acceleration_z,battery,humidity,pressure,temperature,dev-id
1523145619153,2018-04-08T00:00:19.153000Z,1014.1558065701739,-44,324,960,3007,20.5,1017.15,28.37,C2:9A:9F:E5:58:27
1523145624353,2018-04-08T00:00:24.352999Z,1011.8102588924467,-48,316,960,3007,20.5,1017.14,28.38,C2:9A:9F:E5:58:27
1523145629373,2018-04-08T00:00:29.372999Z,1020.4704797298156,-44,320,968,3007,20.5,1017.16,28.38,C2:9A:9F:E5:58:27 …   Out of all the listed application domains (MongoDB, ElasticSearch, Cassandra, CockroachDB, Apache Hive) Cassandra appeared the most suitable not only because it is the only one i was familiar with through the course tutorials, but because of its general suitability for big data applications thanks to its ability to scale, possibility of reliability configurations and write-performance. When tenants constantly write the data ability to distribute the nodes benefits the application by providing high accessibility of the database that in the end prevents data loss. For time series data we can easily divide the data in columns by it, and then efficiency query the data based on the timestamps. The IoT data is also “standardised” (has constant structure). There are also many other features such as data expiration and compression which can become handy when dealing with constantly growing input of iot data, say if it is being sent every second, wouldn’t it be good idea to compress or even completely delete the data that has been around for a long time and probably unneeded at the moment. The CQL is very similar to SQL which i am already familiar with was also a bonus. Personally i would prefer InfluxDB for the tortoise data, but it was not an option.

2. In mysimbdp the key components are mysimbdp-coredms (responsible for storing and managing the data), -dataingest (responsible for reading data from files/messaging systems and ingesting into coredms) and -daas (from which the API’s are supposed to be called to read/write the data to/from the database).  This system can be configured to interact with Apache Nifi. With NiFi we can configure a dataingestion flow for mysimbd-dataingest, that can  interact with mysimbdp-coredms (running cassandra) by configuring a connection to it (address:port) and providing credentials to be able to execute cql commands remotely on the cluster. In the same way, with NiFi, mysimmdp-daas can be deployed to interact with mysimbdp-coredms as part of a single flow having at both ends “processors” handling calls to APIs in form of a HandleHttpRequest -processor at one end and HandleHttpResponse at the other. The third parties i can think of could be some kind of messaging brokers put in between the IoT device and the receiver node of the dataingest.  3. For the custer of Cassandra nodes we use configuration of at least three nodes to provide high availability and fault tolerance. In this scenario the prevention of a single-point-of-failure situation implies that data is replicated among the nodes and that at least one node is available.  4.  The replication level of three (among the three ndoes) would be a sufficient level of redundancy if a node fails, so that the data is possible to retrieve or upload from/to another. In Cassandra this can be easily configured to automatically replicate data among the nodes.   5. 
